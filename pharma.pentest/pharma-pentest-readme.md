# Pharmacy LLM Assistant Penetration Testing Framework

This repository contains tools for performing prompt injection testing against the Pharmacy E-commerce LLM Assistant. It provides an adapter for running Lakera-style attacks through a ZenGuard-compatible testing framework.

## Overview

The framework allows you to:

1. Load and convert custom Lakera-format prompt injection attacks
2. Run these attacks against the Pharmacy E-commerce LLM Assistant API
3. Score and evaluate the effectiveness of attacks in both SAFE and UNSAFE security modes
4. Generate comprehensive reports and visualizations of security vulnerabilities

## Quick Start

### Prerequisites

- Python 3.8+
- Pharmacy E-commerce LLM Assistant backend running
- Lakera-style prompt attack dataset

### Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/yourusername/D-monstrateur-d-attaques-LLM.git
   cd D-monstrateur-d-attaques-LLM/pharma_pentest
   ```

2. **Install dependencies**:
   ```bash
   pip install -r requirements.txt
   ```

3. **Prepare your Lakera custom prompts file**:
   - Place your `lakera_custom_prompts.json` file in the `data/` directory
   - Ensure the file follows the expected format with attack_type, complexity, and expected_leakage fields

### Running Tests

Execute the penetration test script:

```bash
python run_pentest.py
```

This will:
- Load the Lakera-format prompts
- Convert them to ZenGuard-compatible format
- Run them against the LLM Assistant in both SAFE and UNSAFE modes
- Score the results and generate CSV reports
- Create visualizations of attack success rates

## Output Files

The script generates the following output files:

- `results/csv/pentest_results_safe.csv`: Results from the SAFE mode tests
- `results/csv/pentest_results_unsafe.csv`: Results from the UNSAFE mode tests
- `results/csv/pentest_results_combined.csv`: Combined results from both modes
- `results/visualizations/pentest_results.png`: Visualizations of attack success rates

## Adapter Framework

The `PharmaToZenguardAdapter` class handles:

- Loading and filtering Lakera-format prompts
- Converting them to ZenGuard-compatible format
- Running tests against the Pharmacy E-commerce LLM Assistant API
- Scoring results based on expected leakage
- Generating summary statistics and reports

## Customization

You can modify the following parameters in `run_pentest.py`:

- `api_url`: URL of the Pharmacy E-commerce LLM Assistant API
- `lakera_file_path`: Path to the Lakera custom prompts file
- Request timeout and delay settings
- Visualization settings

## Security Notes

- This tool is designed for authorized security testing only
- Test only on systems you have permission to test
- Do not use on production systems without proper authorization

## Troubleshooting

- If you encounter timeouts, adjust the timeout value in `run_tests_with_delay` function
- If tests run too quickly and overwhelm the server, increase the delay between requests
- Check the API URL is correct and the backend server is running
